{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f376e0f",
   "metadata": {},
   "source": [
    "# Multi-Dataset Model Comparison\n",
    "## Experiments on 4 Datasets with Type I Noise + Variable Label Noise\n",
    "Pre-generated datasets contain Type I boundary noise (5%, 10%, 15%, 20%), and we inject label noise at various levels (0%, 5%, 10%, 15%, 20%, 25%, 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datasets import inject_noise\n",
    "from svm_models import NaiveSVM, ProbSVM, KNNSVM, SKiP\n",
    "from multi_svm import OneVsRestSVM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44410313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configurations\n",
    "datasets_config = {\n",
    "    'breast_cancer': {\n",
    "        'clean': 'datasets/breast_cancer/breast_cancer.npz',\n",
    "        'noisy': {\n",
    "            '5%': 'datasets/breast_cancer/fast_breast_cancer_type1_boundary_5pct.npz',\n",
    "            '10%': 'datasets/breast_cancer/fast_breast_cancer_type1_boundary_10pct.npz',\n",
    "            '15%': 'datasets/breast_cancer/fast_breast_cancer_type1_boundary_15pct.npz',\n",
    "            '20%': 'datasets/breast_cancer/fast_breast_cancer_type1_boundary_20pct.npz',\n",
    "        }\n",
    "    },\n",
    "    'iris_2feat': {\n",
    "        'clean': 'datasets/iris_2feat/iris_2feat.npz',\n",
    "        'noisy': {\n",
    "            '5%': 'datasets/iris_2feat/fast_iris_2feat_type1_boundary_5pct.npz',\n",
    "            '10%': 'datasets/iris_2feat/fast_iris_2feat_type1_boundary_10pct.npz',\n",
    "            '15%': 'datasets/iris_2feat/fast_iris_2feat_type1_boundary_15pct.npz',\n",
    "            '20%': 'datasets/iris_2feat/fast_iris_2feat_type1_boundary_20pct.npz',\n",
    "        }\n",
    "    },\n",
    "    'titanic': {\n",
    "        'clean': 'datasets/titanic/titanic.npz',\n",
    "        'noisy': {\n",
    "            '5%': 'datasets/titanic/fast_titanic_type1_boundary_5pct.npz',\n",
    "            '10%': 'datasets/titanic/fast_titanic_type1_boundary_10pct.npz',\n",
    "            '15%': 'datasets/titanic/fast_titanic_type1_boundary_15pct.npz',\n",
    "            '20%': 'datasets/titanic/fast_titanic_type1_boundary_20pct.npz',\n",
    "        }\n",
    "    },\n",
    "    'wine': {\n",
    "        'clean': 'datasets/wine/wine.npz',\n",
    "        'noisy': {\n",
    "            '5%': 'datasets/wine/fast_wine_type1_boundary_5pct.npz',\n",
    "            '10%': 'datasets/wine/fast_wine_type1_boundary_10pct.npz',\n",
    "            '15%': 'datasets/wine/fast_wine_type1_boundary_15pct.npz',\n",
    "            '20%': 'datasets/wine/fast_wine_type1_boundary_20pct.npz',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Label noise levels to inject at runtime\n",
    "label_noise_levels = [0.0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]\n",
    "\n",
    "print(\"Dataset configurations loaded:\")\n",
    "for name in datasets_config.keys():\n",
    "    print(f\"  - {name}\")\n",
    "print(f\"\\nFeature noise levels (pre-generated): Clean, 5%, 10%, 15%, 20% (Type I boundary noise)\")\n",
    "print(f\"Label noise levels (runtime injection): {[f'{int(n*100)}%' for n in label_noise_levels]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83984dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and prepare data\n",
    "def load_dataset(dataset_name, feature_noise_level=None, label_noise=0.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Load dataset from npz file and inject label noise.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_name: Name of the dataset ('breast_cancer', 'iris_2feat', 'titanic', 'wine')\n",
    "    - feature_noise_level: None for clean data, or '5%', '10%', '15%', '20%' for pre-generated feature noise\n",
    "    - label_noise: Proportion of labels to flip (0.0 to 1.0)\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test (all scaled)\n",
    "    \"\"\"\n",
    "    if feature_noise_level is None:\n",
    "        file_path = datasets_config[dataset_name]['clean']\n",
    "    else:\n",
    "        file_path = datasets_config[dataset_name]['noisy'][feature_noise_level]\n",
    "    \n",
    "    data = np.load(file_path)\n",
    "    X = data['X_train']  # The full dataset is stored as X_train\n",
    "    y = data['y_train']  # The full labels are stored as y_train\n",
    "    \n",
    "    # Inject label noise if needed\n",
    "    if label_noise > 0:\n",
    "        X, y = inject_noise(X, y, feature_noise=0.0, label_noise=label_noise, \n",
    "                           random_state=random_state, add_label_noise=False)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "# Test loading\n",
    "dataset_name = 'wine'\n",
    "X_train, X_test, y_train, y_test = load_dataset(dataset_name)\n",
    "print(f\"Test loading: {dataset_name}\")\n",
    "print(f\"  Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"  Classes: {np.unique(y_train)}\")\n",
    "print(f\"  Class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "# Test with label noise\n",
    "X_train_noisy, X_test_noisy, y_train_noisy, y_test_noisy = load_dataset(\n",
    "    dataset_name, feature_noise_level='10%', label_noise=0.10\n",
    ")\n",
    "print(f\"\\nTest loading with 10% feature noise + 10% label noise: {dataset_name}\")\n",
    "print(f\"  Train: {X_train_noisy.shape}, Test: {X_test_noisy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f17f8",
   "metadata": {},
   "source": [
    "## 2. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter configurations\n",
    "C_values = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "k_values = [3, 5, 7]\n",
    "feature_noise_levels = [None, '5%', '10%', '15%', '20%']  # None = clean data\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "print(f\"  C values: {C_values}\")\n",
    "print(f\"  k values (for KNNSVM/SKiP): {k_values}\")\n",
    "print(f\"  Feature noise levels: {['Clean' if x is None else x for x in feature_noise_levels]}\")\n",
    "print(f\"  Label noise levels: {[f'{int(n*100)}%' for n in label_noise_levels]}\")\n",
    "print(f\"  Models: NaiveSVM, ProbSVM, KNNSVM, SKiP (multiply, multiply-minmax, average, average-minmax)\")\n",
    "print(f\"  Kernel: Linear\")\n",
    "print(f\"\\nTotal experiments per dataset:\")\n",
    "num_base_models = len(C_values) * 2  # NaiveSVM, ProbSVM\n",
    "num_knn_models = len(C_values) * len(k_values) * 5  # KNNSVM + 4 SKiP variants\n",
    "total_per_noise_combo = num_base_models + num_knn_models\n",
    "print(f\"  Per feature-label noise combination: {total_per_noise_combo} experiments\")\n",
    "print(f\"  Total per dataset: {total_per_noise_combo * len(feature_noise_levels) * len(label_noise_levels)} experiments\")\n",
    "print(f\"  Grand total (4 datasets): {total_per_noise_combo * len(feature_noise_levels) * len(label_noise_levels) * 4} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeaa9ba",
   "metadata": {},
   "source": [
    "## 3. Run Experiments on All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed814df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main experiment function\n",
    "def run_experiments(dataset_name, feature_noise_level=None, label_noise=0.0, random_state=42):\n",
    "    \"\"\"\n",
    "    Run all model experiments on a specific dataset with given noise levels.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_name: Name of the dataset\n",
    "    - feature_noise_level: None for clean data, or '5%', '10%', '15%', '20%' for pre-generated feature noise\n",
    "    - label_noise: Proportion of labels to flip (0.0 to 1.0)\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - List of result dictionaries\n",
    "    \"\"\"\n",
    "    # Load data with feature noise and inject label noise\n",
    "    X_train, X_test, y_train, y_test = load_dataset(dataset_name, feature_noise_level, label_noise, random_state)\n",
    "    \n",
    "    results = []\n",
    "    feature_noise_str = \"Clean\" if feature_noise_level is None else feature_noise_level\n",
    "    label_noise_str = f\"{int(label_noise * 100)}%\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Dataset: {dataset_name} | Feature Noise: {feature_noise_str} | Label Noise: {label_noise_str}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Test each model with different C values\n",
    "    for C in C_values:\n",
    "        # NaiveSVM\n",
    "        clf = OneVsRestSVM(NaiveSVM(C=C, kernel='linear', verbose=False))\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_acc = (clf.predict(X_train) == y_train).mean()\n",
    "        test_acc = (clf.predict(X_test) == y_test).mean()\n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Feature_Noise': feature_noise_str,\n",
    "            'Label_Noise': label_noise_str,\n",
    "            'Model': 'NaiveSVM',\n",
    "            'C': C,\n",
    "            'k': None,\n",
    "            'Train Acc': train_acc,\n",
    "            'Test Acc': test_acc\n",
    "        })\n",
    "        \n",
    "        # ProbSVM\n",
    "        clf = OneVsRestSVM(ProbSVM(C=C, kernel='linear', verbose=False))\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_acc = (clf.predict(X_train) == y_train).mean()\n",
    "        test_acc = (clf.predict(X_test) == y_test).mean()\n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Feature_Noise': feature_noise_str,\n",
    "            'Label_Noise': label_noise_str,\n",
    "            'Model': 'ProbSVM',\n",
    "            'C': C,\n",
    "            'k': None,\n",
    "            'Train Acc': train_acc,\n",
    "            'Test Acc': test_acc\n",
    "        })\n",
    "    \n",
    "    # Test KNN-based models with different C and k values\n",
    "    for C in C_values:\n",
    "        for k in k_values:\n",
    "            # KNNSVM\n",
    "            clf = OneVsRestSVM(KNNSVM(C=C, k=k, kernel='linear', verbose=False))\n",
    "            clf.fit(X_train, y_train)\n",
    "            train_acc = (clf.predict(X_train) == y_train).mean()\n",
    "            test_acc = (clf.predict(X_test) == y_test).mean()\n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Feature_Noise': feature_noise_str,\n",
    "                'Label_Noise': label_noise_str,\n",
    "                'Model': 'KNNSVM',\n",
    "                'C': C,\n",
    "                'k': k,\n",
    "                'Train Acc': train_acc,\n",
    "                'Test Acc': test_acc\n",
    "            })\n",
    "            \n",
    "            # SKiP - multiply\n",
    "            clf = OneVsRestSVM(SKiP(C=C, k=k, kernel='linear', verbose=False, combine_method='multiply'))\n",
    "            clf.fit(X_train, y_train)\n",
    "            train_acc = (clf.predict(X_train) == y_train).mean()\n",
    "            test_acc = (clf.predict(X_test) == y_test).mean()\n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Feature_Noise': feature_noise_str,\n",
    "                'Label_Noise': label_noise_str,\n",
    "                'Model': 'SKiP-multiply',\n",
    "                'C': C,\n",
    "                'k': k,\n",
    "                'Train Acc': train_acc,\n",
    "                'Test Acc': test_acc\n",
    "            })\n",
    "\n",
    "            # SKiP - multiply (min-max scaling)\n",
    "            clf = OneVsRestSVM(SKiP(C=C, k=k, kernel='linear', verbose=False, combine_method='multiply', scaling='minmax'))\n",
    "            clf.fit(X_train, y_train)\n",
    "            train_acc = (clf.predict(X_train) == y_train).mean()\n",
    "            test_acc = (clf.predict(X_test) == y_test).mean()\n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Feature_Noise': feature_noise_str,\n",
    "                'Label_Noise': label_noise_str,\n",
    "                'Model': 'SKiP-multiply-minmax',\n",
    "                'C': C,\n",
    "                'k': k,\n",
    "                'Train Acc': train_acc,\n",
    "                'Test Acc': test_acc\n",
    "            })\n",
    "\n",
    "            # SKiP - average\n",
    "            clf = OneVsRestSVM(SKiP(C=C, k=k, kernel='linear', verbose=False, combine_method='average'))\n",
    "            clf.fit(X_train, y_train)\n",
    "            train_acc = (clf.predict(X_train) == y_train).mean()\n",
    "            test_acc = (clf.predict(X_test) == y_test).mean()\n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Feature_Noise': feature_noise_str,\n",
    "                'Label_Noise': label_noise_str,\n",
    "                'Model': 'SKiP-average',\n",
    "                'C': C,\n",
    "                'k': k,\n",
    "                'Train Acc': train_acc,\n",
    "                'Test Acc': test_acc\n",
    "            })\n",
    "\n",
    "            # SKiP - average (min-max scaling)\n",
    "            clf = OneVsRestSVM(SKiP(C=C, k=k, kernel='linear', verbose=False, combine_method='average', scaling='minmax'))\n",
    "            clf.fit(X_train, y_train)\n",
    "            train_acc = (clf.predict(X_train) == y_train).mean()\n",
    "            test_acc = (clf.predict(X_test) == y_test).mean()\n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Feature_Noise': feature_noise_str,\n",
    "                'Label_Noise': label_noise_str,\n",
    "                'Model': 'SKiP-average-minmax',\n",
    "                'C': C,\n",
    "                'k': k,\n",
    "                'Train Acc': train_acc,\n",
    "                'Test Acc': test_acc\n",
    "            })\n",
    "    \n",
    "    print(f\"Completed: {len(results)} experiments\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb732016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ALL experiments on ALL datasets\n",
    "all_results = []\n",
    "\n",
    "for dataset_name in ['breast_cancer', 'iris_2feat', 'titanic', 'wine']:\n",
    "    print(f\"\\n\\n{'#'*70}\")\n",
    "    print(f\"# DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    # Test all combinations of feature noise and label noise\n",
    "    for feature_noise_level in feature_noise_levels:\n",
    "        for label_noise in label_noise_levels:\n",
    "            results = run_experiments(dataset_name, feature_noise_level, label_noise, random_state=42)\n",
    "            all_results.extend(results)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_all_results = pd.DataFrame(all_results)\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(f\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total experiments: {len(df_all_results)}\")\n",
    "print(f\"Datasets: {df_all_results['Dataset'].nunique()}\")\n",
    "print(f\"Models: {df_all_results['Model'].nunique()}\")\n",
    "print(f\"Feature noise levels: {df_all_results['Feature_Noise'].nunique()}\")\n",
    "print(f\"Label noise levels: {df_all_results['Label_Noise'].nunique()}\")\n",
    "\n",
    "# Display sample results\n",
    "print(f\"\\nSample results:\")\n",
    "print(df_all_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10d1ab",
   "metadata": {},
   "source": [
    "## 4. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2606891",
   "metadata": {},
   "source": [
    "### 4.1 Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09562c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_filename = 'model_comparison_results.csv'\n",
    "df_all_results.to_csv(output_filename, index=False)\n",
    "print(f\"Results saved to {output_filename}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(df_all_results.groupby('Dataset').size())\n",
    "print(f\"\\nModel statistics:\")\n",
    "print(df_all_results.groupby('Model').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dae7138",
   "metadata": {},
   "source": [
    "### 4.2 Best Model Selection per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing configuration for each dataset and noise combination\n",
    "best_results = df_all_results.loc[df_all_results.groupby(['Dataset', 'Feature_Noise', 'Label_Noise'])['Test Acc'].idxmax()]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BEST MODEL FOR EACH DATASET AND NOISE COMBINATION\")\n",
    "print(\"=\" * 80)\n",
    "for dataset in best_results['Dataset'].unique():\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    dataset_best = best_results[best_results['Dataset'] == dataset]\n",
    "    for _, row in dataset_best.iterrows():\n",
    "        print(f\"  Feature Noise: {row['Feature_Noise']:5s} | Label Noise: {row['Label_Noise']:3s} | \"\n",
    "              f\"Model: {row['Model']:20s} | C: {row['C']:8.1f} | k: {str(row['k']):4s} | \"\n",
    "              f\"Test Acc: {row['Test Acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0925ad",
   "metadata": {},
   "source": [
    "### 4.3 Model Performance Comparison Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average performance across all datasets for clean data\n",
    "clean_data = df_all_results[(df_all_results['Feature_Noise'] == 'Clean') & \n",
    "                            (df_all_results['Label_Noise'] == '0%')]\n",
    "\n",
    "# Get best C for each model\n",
    "model_avg_performance = []\n",
    "for model in clean_data['Model'].unique():\n",
    "    model_data = clean_data[clean_data['Model'] == model]\n",
    "    best_by_dataset = model_data.loc[model_data.groupby('Dataset')['Test Acc'].idxmax()]\n",
    "    avg_test_acc = best_by_dataset['Test Acc'].mean()\n",
    "    model_avg_performance.append({\n",
    "        'Model': model,\n",
    "        'Avg Test Acc': avg_test_acc\n",
    "    })\n",
    "\n",
    "perf_df = pd.DataFrame(model_avg_performance).sort_values('Avg Test Acc', ascending=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AVERAGE PERFORMANCE ACROSS ALL DATASETS (CLEAN DATA)\")\n",
    "print(\"=\" * 60)\n",
    "print(perf_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['red' if 'Naive' in m else 'blue' if 'Prob' in m else 'green' if 'KNN' in m else 'purple' \n",
    "          for m in perf_df['Model']]\n",
    "bars = ax.bar(range(len(perf_df)), perf_df['Avg Test Acc'], color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, acc) in enumerate(zip(bars, perf_df['Avg Test Acc'])):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Average Test Accuracy', fontsize=12)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_title('Average Model Performance Across All Datasets (Clean Data)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(len(perf_df)))\n",
    "ax.set_xticklabels(perf_df['Model'], rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1a401",
   "metadata": {},
   "source": [
    "### 4.4 Noise Robustness Analysis: Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dfff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmaps for each dataset showing noise impact on best model performance\n",
    "for dataset_name in ['breast_cancer', 'iris_2feat', 'titanic', 'wine']:\n",
    "    dataset_results = df_all_results[df_all_results['Dataset'] == dataset_name]\n",
    "    \n",
    "    # Get best performance for each noise combination and model\n",
    "    best_per_combo = dataset_results.loc[\n",
    "        dataset_results.groupby(['Feature_Noise', 'Label_Noise', 'Model'])['Test Acc'].idxmax()\n",
    "    ]\n",
    "    \n",
    "    # Focus on main models - include all SKiP variants\n",
    "    main_models = ['NaiveSVM', 'ProbSVM', 'KNNSVM', \n",
    "                   'SKiP-multiply', 'SKiP-multiply-minmax',\n",
    "                   'SKiP-average', 'SKiP-average-minmax']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, model in enumerate(main_models):\n",
    "        model_data = best_per_combo[best_per_combo['Model'] == model]\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot = model_data.pivot_table(\n",
    "            values='Test Acc',\n",
    "            index='Label_Noise',\n",
    "            columns='Feature_Noise',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Reorder columns and index\n",
    "        feature_order = ['Clean', '5%', '10%', '15%', '20%']\n",
    "        label_order = ['0%', '5%', '10%', '15%', '20%', '25%', '30%']\n",
    "        pivot = pivot.reindex(index=label_order, columns=feature_order)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        ax = axes[idx]\n",
    "        im = ax.imshow(pivot.values, cmap='RdYlGn', aspect='auto', vmin=0.5, vmax=1.0)\n",
    "        \n",
    "        # Set ticks\n",
    "        ax.set_xticks(range(len(feature_order)))\n",
    "        ax.set_yticks(range(len(label_order)))\n",
    "        ax.set_xticklabels(feature_order)\n",
    "        ax.set_yticklabels(label_order)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(label_order)):\n",
    "            for j in range(len(feature_order)):\n",
    "                if not np.isnan(pivot.values[i, j]):\n",
    "                    text = ax.text(j, i, f'{pivot.values[i, j]:.3f}',\n",
    "                                 ha='center', va='center', color='black', fontsize=7)\n",
    "        \n",
    "        ax.set_title(f'{model}', fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Feature Noise', fontsize=9)\n",
    "        ax.set_ylabel('Label Noise', fontsize=9)\n",
    "    \n",
    "    # Hide the last unused subplot\n",
    "    axes[-1].set_visible(False)\n",
    "    \n",
    "    # Add colorbar\n",
    "    fig.colorbar(im, ax=axes[:-1], orientation='horizontal', pad=0.05, fraction=0.05, label='Test Accuracy')\n",
    "    \n",
    "    plt.suptitle(f'Noise Impact on Test Accuracy - {dataset_name.upper()}', \n",
    "                 fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'noise_heatmap_{dataset_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved heatmap for {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f55f5",
   "metadata": {},
   "source": [
    "### 4.5 Performance Degradation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bee4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance degradation from clean to noisy data\n",
    "degradation_analysis = []\n",
    "\n",
    "for dataset in df_all_results['Dataset'].unique():\n",
    "    # Get clean baseline (no feature noise, no label noise)\n",
    "    clean_baseline = df_all_results[\n",
    "        (df_all_results['Dataset'] == dataset) & \n",
    "        (df_all_results['Feature_Noise'] == 'Clean') & \n",
    "        (df_all_results['Label_Noise'] == '0%')\n",
    "    ]\n",
    "    \n",
    "    # Get maximum noise (20% feature, 30% label)\n",
    "    max_noise = df_all_results[\n",
    "        (df_all_results['Dataset'] == dataset) & \n",
    "        (df_all_results['Feature_Noise'] == '20%') & \n",
    "        (df_all_results['Label_Noise'] == '30%')\n",
    "    ]\n",
    "    \n",
    "    for model in clean_baseline['Model'].unique():\n",
    "        clean_acc = clean_baseline[clean_baseline['Model'] == model]['Test Acc'].max()\n",
    "        noisy_acc = max_noise[max_noise['Model'] == model]['Test Acc'].max()\n",
    "        degradation = clean_acc - noisy_acc\n",
    "        \n",
    "        degradation_analysis.append({\n",
    "            'Dataset': dataset,\n",
    "            'Model': model,\n",
    "            'Clean Acc': clean_acc,\n",
    "            'Max Noise Acc': noisy_acc,\n",
    "            'Degradation': degradation,\n",
    "            'Degradation %': (degradation / clean_acc) * 100\n",
    "        })\n",
    "\n",
    "degradation_df = pd.DataFrame(degradation_analysis)\n",
    "\n",
    "# Visualize degradation - include all SKiP variants\n",
    "models_to_plot = ['NaiveSVM', 'ProbSVM', 'KNNSVM', \n",
    "                  'SKiP-multiply', 'SKiP-multiply-minmax', \n",
    "                  'SKiP-average', 'SKiP-average-minmax']\n",
    "plot_data = degradation_df[degradation_df['Model'].isin(models_to_plot)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, dataset in enumerate(['breast_cancer', 'iris_2feat', 'titanic', 'wine']):\n",
    "    ax = axes[idx]\n",
    "    dataset_data = plot_data[plot_data['Dataset'] == dataset]\n",
    "    \n",
    "    x = np.arange(len(models_to_plot))\n",
    "    width = 0.35\n",
    "    \n",
    "    clean_accs = [dataset_data[dataset_data['Model'] == m]['Clean Acc'].values[0] \n",
    "                  for m in models_to_plot]\n",
    "    noisy_accs = [dataset_data[dataset_data['Model'] == m]['Max Noise Acc'].values[0] \n",
    "                  for m in models_to_plot]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, clean_accs, width, label='Clean', \n",
    "                   color='green', alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, noisy_accs, width, label='Max Noise (20% feat + 30% label)',\n",
    "                   color='red', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "    \n",
    "    ax.set_ylabel('Test Accuracy', fontsize=10)\n",
    "    ax.set_title(f'{dataset.upper()}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models_to_plot, rotation=45, ha='right', fontsize=8)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0.5, 1.05])\n",
    "\n",
    "plt.suptitle('Performance Degradation: Clean vs Maximum Noise', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_degradation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE DEGRADATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "for dataset in degradation_df['Dataset'].unique():\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    dataset_deg = degradation_df[degradation_df['Dataset'] == dataset].sort_values('Degradation')\n",
    "    print(dataset_deg[['Model', 'Clean Acc', 'Max Noise Acc', 'Degradation', 'Degradation %']].to_string(index=False))\n",
    "    print(f\"\\nMost robust model: {dataset_deg.iloc[0]['Model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef857bf",
   "metadata": {},
   "source": [
    "### 4.6 Hyperparameter Analysis: C and k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze impact of C parameter across different noise levels for all SKiP variants\n",
    "dataset_to_analyze = 'wine'\n",
    "skip_variants = ['SKiP-multiply', 'SKiP-multiply-minmax', 'SKiP-average', 'SKiP-average-minmax']\n",
    "\n",
    "noise_combos = [\n",
    "    ('Clean', '0%'), ('Clean', '30%'),\n",
    "    ('10%', '0%'), ('10%', '30%'),\n",
    "    ('20%', '0%'), ('20%', '30%')\n",
    "]\n",
    "\n",
    "# C parameter analysis for each SKiP variant\n",
    "for model_to_analyze in skip_variants:\n",
    "    model_data = df_all_results[\n",
    "        (df_all_results['Dataset'] == dataset_to_analyze) & \n",
    "        (df_all_results['Model'] == model_to_analyze) &\n",
    "        (df_all_results['k'] == 5)  # Fix k for analysis\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, (feat_noise, label_noise) in enumerate(noise_combos):\n",
    "        ax = axes[idx]\n",
    "        combo_data = model_data[\n",
    "            (model_data['Feature_Noise'] == feat_noise) & \n",
    "            (model_data['Label_Noise'] == label_noise)\n",
    "        ]\n",
    "        \n",
    "        # Group by C and take max\n",
    "        c_performance = combo_data.groupby('C')['Test Acc'].max()\n",
    "        \n",
    "        ax.plot(c_performance.index, c_performance.values, marker='o', linewidth=2, markersize=8)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlabel('C (log scale)', fontsize=10)\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=10)\n",
    "        ax.set_title(f'Feature: {feat_noise}, Label: {label_noise}', fontsize=11, fontweight='bold')\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_ylim([0.80, 1.0])\n",
    "    \n",
    "    plt.suptitle(f'Impact of C Parameter on {model_to_analyze} - {dataset_to_analyze.upper()}', \n",
    "                 fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'hyperparameter_C_{dataset_to_analyze}_{model_to_analyze}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# k parameter analysis for all SKiP variants\n",
    "for model_to_analyze in skip_variants:\n",
    "    model_data = df_all_results[\n",
    "        (df_all_results['Dataset'] == dataset_to_analyze) & \n",
    "        (df_all_results['Model'] == model_to_analyze) &\n",
    "        (df_all_results['C'] == 10.0)  # Fix C for analysis\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, (feat_noise, label_noise) in enumerate(noise_combos):\n",
    "        ax = axes[idx]\n",
    "        combo_data = model_data[\n",
    "            (model_data['Feature_Noise'] == feat_noise) & \n",
    "            (model_data['Label_Noise'] == label_noise)\n",
    "        ]\n",
    "        \n",
    "        # Group by k and take mean\n",
    "        k_performance = combo_data.groupby('k')['Test Acc'].mean()\n",
    "        \n",
    "        ax.plot(k_performance.index, k_performance.values, marker='s', linewidth=2, markersize=8, color='orange')\n",
    "        ax.set_xlabel('k (number of neighbors)', fontsize=10)\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=10)\n",
    "        ax.set_title(f'Feature: {feat_noise}, Label: {label_noise}', fontsize=11, fontweight='bold')\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_ylim([0.80, 1.0])\n",
    "        ax.set_xticks([3, 5, 7])\n",
    "    \n",
    "    plt.suptitle(f'Impact of k Parameter on {model_to_analyze} - {dataset_to_analyze.upper()}', \n",
    "                 fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'hyperparameter_k_{dataset_to_analyze}_{model_to_analyze}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830108d8",
   "metadata": {},
   "source": [
    "### 4.7 SKiP Variants Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897aeee",
   "metadata": {},
   "source": [
    "### 4.6.1 All SKiP Variants Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all SKiP variants across different noise levels on one dataset\n",
    "dataset_to_analyze = 'wine'\n",
    "skip_variants = ['SKiP-multiply', 'SKiP-multiply-minmax', 'SKiP-average', 'SKiP-average-minmax']\n",
    "\n",
    "# For each noise combination, compare all SKiP variants\n",
    "noise_combos_to_plot = [\n",
    "    ('Clean', '0%'), ('Clean', '15%'), ('Clean', '30%'),\n",
    "    ('10%', '0%'), ('10%', '15%'), ('10%', '30%'),\n",
    "    ('20%', '0%'), ('20%', '15%'), ('20%', '30%')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (feat_noise, label_noise) in enumerate(noise_combos_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    variant_perfs = []\n",
    "    for variant in skip_variants:\n",
    "        variant_data = df_all_results[\n",
    "            (df_all_results['Dataset'] == dataset_to_analyze) &\n",
    "            (df_all_results['Model'] == variant) &\n",
    "            (df_all_results['Feature_Noise'] == feat_noise) &\n",
    "            (df_all_results['Label_Noise'] == label_noise)\n",
    "        ]\n",
    "        best_acc = variant_data['Test Acc'].max()\n",
    "        variant_perfs.append(best_acc)\n",
    "    \n",
    "    x = np.arange(len(skip_variants))\n",
    "    colors = ['blue', 'cyan', 'orange', 'coral']\n",
    "    bars = ax.bar(x, variant_perfs, color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, perf in zip(bars, variant_perfs):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                f'{perf:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('Test Accuracy', fontsize=9)\n",
    "    ax.set_title(f'Feat: {feat_noise}, Label: {label_noise}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([v.replace('SKiP-', '') for v in skip_variants], \n",
    "                       rotation=45, ha='right', fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0.85, 1.0])\n",
    "\n",
    "plt.suptitle(f'All SKiP Variants Performance Comparison - {dataset_to_analyze.upper()}', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'skip_all_variants_comparison_{dataset_to_analyze}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a337f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different SKiP variants\n",
    "skip_variants = ['SKiP-multiply', 'SKiP-multiply-minmax', 'SKiP-average', 'SKiP-average-minmax']\n",
    "\n",
    "# Get best performance for each variant across all datasets\n",
    "variant_comparison = []\n",
    "for dataset in df_all_results['Dataset'].unique():\n",
    "    for variant in skip_variants:\n",
    "        variant_data = df_all_results[\n",
    "            (df_all_results['Dataset'] == dataset) & \n",
    "            (df_all_results['Model'] == variant)\n",
    "        ]\n",
    "        \n",
    "        # Clean data performance\n",
    "        clean_perf = variant_data[\n",
    "            (variant_data['Feature_Noise'] == 'Clean') & \n",
    "            (variant_data['Label_Noise'] == '0%')\n",
    "        ]['Test Acc'].max()\n",
    "        \n",
    "        # Maximum noise performance\n",
    "        max_noise_perf = variant_data[\n",
    "            (variant_data['Feature_Noise'] == '20%') & \n",
    "            (variant_data['Label_Noise'] == '30%')\n",
    "        ]['Test Acc'].max()\n",
    "        \n",
    "        variant_comparison.append({\n",
    "            'Dataset': dataset,\n",
    "            'Variant': variant,\n",
    "            'Clean': clean_perf,\n",
    "            'Max Noise': max_noise_perf,\n",
    "            'Degradation': clean_perf - max_noise_perf\n",
    "        })\n",
    "\n",
    "variant_df = pd.DataFrame(variant_comparison)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, dataset in enumerate(['breast_cancer', 'iris_2feat', 'titanic', 'wine']):\n",
    "    ax = axes[idx]\n",
    "    dataset_variants = variant_df[variant_df['Dataset'] == dataset]\n",
    "    \n",
    "    x = np.arange(len(skip_variants))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, dataset_variants['Clean'], width, \n",
    "                   label='Clean', color='blue', alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, dataset_variants['Max Noise'], width,\n",
    "                   label='Max Noise', color='red', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "    \n",
    "    ax.set_ylabel('Test Accuracy', fontsize=10)\n",
    "    ax.set_title(f'{dataset.upper()}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([v.replace('SKiP-', '') for v in skip_variants], \n",
    "                       rotation=45, ha='right', fontsize=9)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0.5, 1.05])\n",
    "\n",
    "plt.suptitle('SKiP Variants Comparison', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('skip_variants_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SKiP VARIANTS COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "for dataset in variant_df['Dataset'].unique():\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    dataset_var = variant_df[variant_df['Dataset'] == dataset].sort_values('Degradation')\n",
    "    print(dataset_var[['Variant', 'Clean', 'Max Noise', 'Degradation']].to_string(index=False))\n",
    "    print(f\"\\nMost robust variant: {dataset_var.iloc[0]['Variant']}\")\n",
    "    print(f\"Best clean performance: {dataset_var.loc[dataset_var['Clean'].idxmax(), 'Variant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b468c",
   "metadata": {},
   "source": [
    "### 4.8 Overall Best Model Across All Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance across all noise conditions for each model\n",
    "overall_performance = []\n",
    "\n",
    "for model in df_all_results['Model'].unique():\n",
    "    model_data = df_all_results[df_all_results['Model'] == model]\n",
    "    \n",
    "    # Get best performance for each dataset/noise combination\n",
    "    best_per_combo = model_data.loc[model_data.groupby(['Dataset', 'Feature_Noise', 'Label_Noise'])['Test Acc'].idxmax()]\n",
    "    \n",
    "    avg_performance = best_per_combo['Test Acc'].mean()\n",
    "    std_performance = best_per_combo['Test Acc'].std()\n",
    "    min_performance = best_per_combo['Test Acc'].min()\n",
    "    max_performance = best_per_combo['Test Acc'].max()\n",
    "    \n",
    "    overall_performance.append({\n",
    "        'Model': model,\n",
    "        'Mean Acc': avg_performance,\n",
    "        'Std Acc': std_performance,\n",
    "        'Min Acc': min_performance,\n",
    "        'Max Acc': max_performance\n",
    "    })\n",
    "\n",
    "overall_df = pd.DataFrame(overall_performance).sort_values('Mean Acc', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"OVERALL MODEL PERFORMANCE ACROSS ALL CONDITIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(overall_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "models_sorted = overall_df['Model'].values\n",
    "means = overall_df['Mean Acc'].values\n",
    "stds = overall_df['Std Acc'].values\n",
    "\n",
    "x = np.arange(len(models_sorted))\n",
    "colors = ['red' if 'Naive' in m else 'blue' if 'Prob' in m else 'green' if 'KNN' in m else 'purple' \n",
    "          for m in models_sorted]\n",
    "\n",
    "bars = ax.bar(x, means, yerr=stds, capsize=5, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., mean + std + 0.01,\n",
    "            f'{mean:.4f}±{std:.4f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Average Test Accuracy', fontsize=12)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_title('Overall Model Performance Across All Datasets and Noise Levels', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_sorted, rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0.75, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('overall_model_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Best overall model: {overall_df.iloc[0]['Model']}\")\n",
    "print(f\"✓ Most consistent model (lowest std): {overall_df.loc[overall_df['Std Acc'].idxmin(), 'Model']}\")\n",
    "print(f\"✓ Highest peak performance: {overall_df.loc[overall_df['Max Acc'].idxmax(), 'Model']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
